{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in chars: 1198747\n",
      "Corpus length in words: 212571\n",
      "Unique words before ignoring: 23624\n",
      "Ignoring words with frequency < 10\n",
      "Unique words after ignoring: 2187\n",
      "Ignored sequences: 193573\n",
      "Remaining sequences: 18988\n",
      "Shuffling sentences\n",
      "Size of training set = 18608\n",
      "Size of test set = 380\n",
      "Build model...\n",
      "Epoch 1/100\n",
      "582/582 [==============================] - 20s - loss: 6.2259 - acc: 0.0666 - val_loss: 6.1484 - val_acc: 0.0573\n",
      "Epoch 2/100\n",
      "582/582 [==============================] - 17s - loss: 5.8007 - acc: 0.0746 - val_loss: 5.9315 - val_acc: 0.0651\n",
      "Epoch 3/100\n",
      "582/582 [==============================] - 17s - loss: 5.4671 - acc: 0.0924 - val_loss: 5.8164 - val_acc: 0.0990\n",
      "Epoch 4/100\n",
      "582/582 [==============================] - 17s - loss: 5.1137 - acc: 0.1180 - val_loss: 5.7886 - val_acc: 0.1172\n",
      "Epoch 5/100\n",
      "582/582 [==============================] - 17s - loss: 4.7295 - acc: 0.1474 - val_loss: 5.8298 - val_acc: 0.1354\n",
      "Epoch 6/100\n",
      "582/582 [==============================] - 16s - loss: 4.2915 - acc: 0.1806 - val_loss: 5.9274 - val_acc: 0.1250\n",
      "Epoch 7/100\n",
      "582/582 [==============================] - 16s - loss: 3.8611 - acc: 0.2203 - val_loss: 6.1198 - val_acc: 0.1172\n",
      "Epoch 8/100\n",
      "582/582 [==============================] - 17s - loss: 3.3754 - acc: 0.2812 - val_loss: 6.2122 - val_acc: 0.1198\n",
      "Epoch 9/100\n",
      "582/582 [==============================] - 17s - loss: 2.9189 - acc: 0.3571 - val_loss: 6.3625 - val_acc: 0.1094\n",
      "Epoch 10/100\n",
      "582/582 [==============================] - 17s - loss: 2.4722 - acc: 0.4393 - val_loss: 6.5551 - val_acc: 0.0964\n",
      "Epoch 11/100\n",
      "582/582 [==============================] - 17s - loss: 2.0722 - acc: 0.5128 - val_loss: 6.9582 - val_acc: 0.0885\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LSTM text generation by word. Using Asimov's Foundation trilogy as corpus,\n",
    "the best val_acc I could get is 0.1354 with these parameters:\n",
    "\n",
    "SEQUENCE_LEN = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "dropout = 0.4\n",
    "LSTM(256)\n",
    "\n",
    "Based on: https://github.com/enriqueav/lstm_lyrics\n",
    "\n",
    "Uses data generator to avoid loading all the test set into memory.\n",
    "Saves the weights and model every epoch.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "SEQUENCE_LEN = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write(\n",
    "            '----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "    # Remove the new lines because they appear randomly\n",
    "    with io.open('corpus.txt', encoding='utf-8') as f:\n",
    "        text = f.read().lower().replace('\\n', '').replace('â€™', '\\'').replace('\"', '')\n",
    "        print('Corpus length in chars:', len(text))\n",
    "\n",
    "    # Split corpus into words\n",
    "    text_in_words = [w for w in text.split(' ') if w.strip() != '']\n",
    "    print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "    word_freq = {}\n",
    "    for word in text_in_words:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "    ignored_words = set()\n",
    "    for k, v in word_freq.items():\n",
    "        if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "            ignored_words.add(k)\n",
    "\n",
    "    words = set(text_in_words)\n",
    "    print('Unique words before ignoring:', len(words))\n",
    "    print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "    words = sorted(set(words) - ignored_words)\n",
    "    print('Unique words after ignoring:', len(words))\n",
    "\n",
    "    word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "    indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "    # cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "    sentences = []\n",
    "    next_words = []\n",
    "    ignored = 0\n",
    "    for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "        # Only add sequences where no word is in ignored_words\n",
    "        if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "            sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "            next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "        else:\n",
    "            ignored = ignored+1\n",
    "\n",
    "    print('Ignored sequences:', ignored)\n",
    "    print('Remaining sequences:', len(sentences))\n",
    "\n",
    "    (sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)\n",
    "\n",
    "    dropout = 0.4\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(256), input_shape=(SEQUENCE_LEN, len(words))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "                \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % \\\n",
    "                (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "    print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "    callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "    examples_file = open('examples.txt', \"w\")\n",
    "    model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                        steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                        epochs=100,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=generator(\n",
    "                            sentences_test, next_words_test, BATCH_SIZE),\n",
    "                        validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SEQUENCE_LEN = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "dropout = 0.2, 0.3\n",
    "LSTM(256)\n",
    "0.1172 on epoch 6\n",
    "<br>\n",
    "SEQUENCE_LEN = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "dropout = 0.4\n",
    "LSTM(256)\n",
    "0.1354 on epoch 6\n",
    "<br>\n",
    "SEQUENCE_LEN = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "dropout = 0.5\n",
    "LSTM(256)\n",
    "0.0859 on epoch 6\n",
    "<br>\n",
    "SEQUENCE_LEN = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "dropout = 0.4\n",
    "LSTM(128)\n",
    "0.0859 on epoch 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
